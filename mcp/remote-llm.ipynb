{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff79041f",
   "metadata": {},
   "source": [
    "# Wetlands LLM + MCP Testing\n",
    "\n",
    "Test the LLM with MCP server integration using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b93a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527d6a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/lib/python312.zip',\n",
       " '/usr/lib/python3.12',\n",
       " '/usr/lib/python3.12/lib-dynload',\n",
       " '',\n",
       " '/home/cboettig/Documents/github/boettiger-lab/wetlands/.venv/lib/python3.12/site-packages']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6b8cd",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ffa32cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Endpoint: https://ellm.nrp-nautilus.io/v1\n",
      "Model: gpt-oss\n",
      "MCP Server: https://biodiversity-mcp.nrp-nautilus.io/mcp\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "import os\n",
    "import json\n",
    "# Use hosted MCP server by default\n",
    "HOSTED_MCP_URL = \"https://biodiversity-mcp.nrp-nautilus.io/\"\n",
    "CONFIG_PATH = 'config.json'\n",
    "config = {}\n",
    "if os.path.exists(CONFIG_PATH):\n",
    "    with open(CONFIG_PATH) as f:\n",
    "        config = json.load(f)\n",
    "mcp_url = config.get('mcp_server_url', HOSTED_MCP_URL)\n",
    "llm_endpoint = config.get('llm_endpoint', None)\n",
    "llm_model = config.get('llm_model', None)\n",
    "\n",
    "# Get API key from environment\n",
    "api_key = os.getenv('NRP_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"WARNING: NRP_API_KEY not set!\")\n",
    "    api_key = input(\"Enter your NRP API key: \")\n",
    "\n",
    "print(f\"LLM Endpoint: {llm_endpoint}\")\n",
    "print(f\"Model: {llm_model}\")\n",
    "print(f\"MCP Server: {mcp_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711bc37f",
   "metadata": {},
   "source": [
    "## Load System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3489be17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt loaded: 5131 characters\n"
     ]
    }
   ],
   "source": [
    "with open('system-prompt.md') as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "print(f\"System prompt loaded: {len(system_prompt)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60654434",
   "metadata": {},
   "source": [
    "## Define MCP Query Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d20fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_mcp(sql_query: str) -> str:\n",
    "    \"\"\"Execute SQL query via MCP server\"\"\"\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": 1,\n",
    "        \"method\": \"tools/call\",\n",
    "        \"params\": {\n",
    "            \"name\": \"query\",\n",
    "            \"arguments\": {\n",
    "                \"query\": sql_query\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(mcp_url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    if 'error' in data:\n",
    "        raise Exception(f\"MCP error: {data['error']['message']}\")\n",
    "    \n",
    "    return data['result']['content'][0]['text']\n",
    "\n",
    "@tool\n",
    "def query_wetlands_data(query: str) -> str:\n",
    "    \"\"\"Execute a SQL query on the wetlands database using DuckDB.\n",
    "    \n",
    "    Args:\n",
    "        query: SQL query to execute. Must start with S3 secret setup.\n",
    "    \n",
    "    Returns:\n",
    "        Query results as a string\n",
    "    \"\"\"\n",
    "    return query_mcp(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ef499",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0e0426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized with tools\n"
     ]
    }
   ],
   "source": [
    "# Initialize LangChain LLM\n",
    "llm = ChatOpenAI(\n",
    "    base_url=llm_endpoint,\n",
    "    api_key=api_key,\n",
    "    model=llm_model,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Bind the tool\n",
    "llm_with_tools = llm.bind_tools([query_wetlands_data])\n",
    "\n",
    "print(\"LLM initialized with tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2dcf69",
   "metadata": {},
   "source": [
    "## Test Direct MCP Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0bb5b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://biodiversity-mcp.nrp-nautilus.io/mcp",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     17\u001b[39m headers = {\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext/plain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m }\n\u001b[32m     21\u001b[39m response = requests.post(mcp_url, data=test_query, headers=headers)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m result = response.json()\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMCP Test Result:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/boettiger-lab/wetlands/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://biodiversity-mcp.nrp-nautilus.io/mcp"
     ]
    }
   ],
   "source": [
    "# Test MCP server directly with plain SQL payload\n",
    "test_query = \"\"\"\n",
    "CREATE OR REPLACE SECRET s3secret (\n",
    "    TYPE S3,\n",
    "    PROVIDER CONFIG,\n",
    "    ENDPOINT 'minio.carlboettiger.info',\n",
    "    USE_SSL true,\n",
    "    URL_STYLE 'path'\n",
    ");\n",
    "\n",
    "SELECT COUNT(*) as total_hexagons \n",
    "FROM read_parquet('s3://public-wetlands/hex/**')\n",
    "WHERE Z > 0;\n",
    "\"\"\"\n",
    "\n",
    "# Send plain SQL directly to MCP server at root URL\n",
    "headers = {\n",
    "    \"Content-Type\": \"text/plain\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "# Remove any trailing /mcp from mcp_url if present\n",
    "mcp_root_url = mcp_url.rstrip('/')\n",
    "if mcp_root_url.endswith('/mcp'):\n",
    "    mcp_root_url = mcp_root_url[:-4]\n",
    "response = requests.post(mcp_root_url, data=test_query, headers=headers)\n",
    "response.raise_for_status()\n",
    "result = response.json()\n",
    "print(\"MCP Test Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbeb2b5",
   "metadata": {},
   "source": [
    "## Test LLM Without Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416614fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test without tools\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is 2+2?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"LLM Test (no tools):\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087eb55b",
   "metadata": {},
   "source": [
    "## Test LLM With Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_tools(user_message: str):\n",
    "    \"\"\"Send message to LLM and handle tool calls\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_message)\n",
    "    ]\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"User: {user_message}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Check if LLM wants to call a tool\n",
    "    if response.tool_calls:\n",
    "        for tool_call in response.tool_calls:\n",
    "            print(f\"üîß LLM is calling tool: {tool_call['name']}\\n\")\n",
    "            \n",
    "            # Extract SQL query from tool call args\n",
    "            args = tool_call.get('args', {})\n",
    "            \n",
    "            # Check if args is empty or doesn't have 'query' key\n",
    "            if not args or 'query' not in args:\n",
    "                print(f\"‚ö†Ô∏è Empty or invalid args from LLM: {args}\")\n",
    "                print(\"LLM failed to provide query. Asking for plain text response instead...\\n\")\n",
    "                \n",
    "                # Get a non-tool response instead\n",
    "                plain_response = llm.invoke(messages)\n",
    "                print(\"ü§ñ Assistant:\")\n",
    "                print(plain_response.content)\n",
    "                return plain_response.content\n",
    "            \n",
    "            sql_query = args['query']\n",
    "            \n",
    "            print(\"SQL Query:\")\n",
    "            print(\"-\" * 60)\n",
    "            print(sql_query)\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            try:\n",
    "                tool_result = query_wetlands_data.invoke({'query': sql_query})\n",
    "                print(\"\\nQuery Result:\")\n",
    "                print(\"-\" * 60)\n",
    "                print(tool_result)\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                # Send result back to LLM for interpretation\n",
    "                messages.append(response)\n",
    "                messages.append(\n",
    "                    HumanMessage(\n",
    "                        content=f\"Tool result: {tool_result}\",\n",
    "                        name=\"tool_result\"\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                final_response = llm.invoke(messages)\n",
    "                print(\"\\nü§ñ Assistant:\")\n",
    "                print(final_response.content)\n",
    "                return final_response.content\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error executing query: {e}\")\n",
    "                print(\"Asking LLM to try again...\\n\")\n",
    "                \n",
    "                # Tell LLM about the error and ask it to try again\n",
    "                messages.append(response)\n",
    "                messages.append(\n",
    "                    HumanMessage(\n",
    "                        content=f\"Error executing query: {e}\\n\\nPlease provide a corrected SQL query.\"\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                retry_response = llm.invoke(messages)\n",
    "                print(\"ü§ñ Assistant (retry):\")\n",
    "                print(retry_response.content)\n",
    "                return retry_response.content\n",
    "    else:\n",
    "        print(\"ü§ñ Assistant:\")\n",
    "        print(response.content)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf612c28",
   "metadata": {},
   "source": [
    "## Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9419197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Total wetlands area\n",
    "chat_with_tools(\"How many hectares of wetlands are there in total?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf9ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Peatlands\n",
    "chat_with_tools(\"What is the total area of peatlands in square kilometers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Freshwater vs saline\n",
    "chat_with_tools(\"Compare the area of freshwater wetlands to saline wetlands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81684dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom query\n",
    "user_query = \"What are the top 5 most common wetland types by area?\"\n",
    "chat_with_tools(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c70fb4",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "If the LLM returns empty args, the model may not support tool calling properly. You can:\n",
    "1. Try a different model (e.g., `gpt-4` or `gpt-3.5-turbo`)\n",
    "2. Manually construct queries and call `query_mcp()` directly\n",
    "3. Use the MCP server test above to verify queries work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
